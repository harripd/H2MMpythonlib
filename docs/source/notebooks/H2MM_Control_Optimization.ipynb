{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e936351e",
   "metadata": {},
   "source": [
    "# `H2MM_C` Secondary Control Features\n",
    "\n",
    "Let's get our obligitory imports in order, and we'll load the 3 detector data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bda2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import H2MM_C as hm\n",
    "\n",
    "# load the data\n",
    "color3 = list()\n",
    "times3 = list()\n",
    "\n",
    "i = 0\n",
    "with open('sample_data_3det.txt','r') as f:\n",
    "    for line in f:\n",
    "        if i % 2 == 0:\n",
    "            times3.append(np.array([int(x) for x in line.split()],dtype='Q'))\n",
    "        else:\n",
    "            color3.append(np.array([int(x) for x in line.split()],dtype='L'))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aee2b4",
   "metadata": {},
   "source": [
    "## Optimization Control\n",
    "Sometimes you want to control when optimizations stop and how many cores the optimization uses.\n",
    "\n",
    "There are 4 distinct limits of this sort:\n",
    "1. Number of cores- `num_cores = os.cpu_count // 2`\n",
    "2. Maximum number of iterations- `max_iter = 3600`\n",
    "3. Minimum number difference between loglik to consider converted- `converged_min = 1e-14`\n",
    "4. Maximum time of optimizatoion- `max_time = np.inf`\n",
    "    > **Note:**\\\n",
    "    > This counter uses the rather inaccurate C clock which tends to run fast, so your optimizations will often end earlier than the number entered.\n",
    "    > The use of this parameter is generally discouraged\n",
    "\n",
    "We'll start by demonstraing the use of `max_iter`.\n",
    "Beging set by default to 3600, this is likely to be enough, but there is always the possibility that the optimization is still improving significantly.\n",
    "\n",
    "Note that the optimization didn't converge when we optimized for 4 states with the 3 detector data.\n",
    "So to increase the number of iterations, we use the keyword argument `max_iter` in `hm.EM_H2MM_C()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8907ea98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Optimization reached maximum number of iterations"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_5s3d = hm.EM_H2MM_C(hm.factory_h2mm_model(4,3), color3, times3, max_iter=7200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f7573",
   "metadata": {},
   "source": [
    "The rest of the limits work in the same way, pass a keyword argument to `hm.EM_H2MM_C()` and over-ride the default.\n",
    "\n",
    "Some notes about each of the limits:\n",
    "- `num_cores` default is set when at import, where python calls `os.cpu_count() // 2` the reason that it uses `// 2` is because most machines are multi-threaded, and `os.cpu_count()` returns the number of cpu threads, and not the number of physical cores. However, model optimizations are cpu-intensive, so the ideal number of threads to use is the number of physical cores, not the nubmer of hyper-threads. If your machine is not hyper-threaded, you'll want to manually set this value. Another possibility is if you want to leave a cpu core or two open for other tasks, you could set `num_cores` to a smaller number.\n",
    "- `max_iter` is the better way to limit the duration of optimizations. Make sure it is high enough, but often when an optimization does not converge quickly, it is because the model is over-fit and has too many states.\n",
    "- `converged_min` sets how close to models have to be to consider a model converged. Due to floating point errors, it is entirely possible that two models with a very close loglik may improve only because of floating point error, or even for the \"real\" better value to be the oposite of what the calculation suggests. Especially if you have  a large data set, you might consider setting this to `1e-7` or similar value (small, but not as small as `1e-14`). It should be noted the differences in values between two models with such similar loglikelihoods will be negligable.\n",
    "- `max_time` is by default infinite, so it is ignored. This is generaly best, as it uses an inaccurate C-clock, only use this if you really need to. The units are in seconds.\n",
    "\n",
    "### Universal Defaults\n",
    "\n",
    "To make it easier to change defaults, `H2MM_C` offers the `optimization_limits` variable, where you can change the default of the 4 optimization limits in the `H2MM_C.optimization_limits` variable, which uses the same syntax as a dictionary.\n",
    "\n",
    "> This was inspired by the plt.rcParams varaible\n",
    "\n",
    "So, for instance, if you know that the ideal number of cores is for instance 2, and not the default automatically supplied, instead of constantly setting the `num_cores` keyword, you can write:\n",
    "\n",
    "`hm.optimization_limits['num_cores'] = 2`\n",
    "\n",
    "At the beginning of the code, and not have to worry about the constantly writing the `num_cores` keyword argument.\n",
    "\n",
    "This works for all the other 3 limits as well, so you can reduce the nubmer of iterations to 1000 with:\n",
    "\n",
    "`hm.optimization_limits['max_iter'] = 1000`\n",
    "\n",
    "Or, if you have many large and similarly sized data sets, and want to make difference needed to consider a model converged a bit larger, you can supply:\n",
    "\n",
    "`hm.optimization_limits['converged_min'] = 1e-7`\n",
    "\n",
    "While possible, as mentioned before, using `max_time` is generally discouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e91a947-21fc-4bd6-bd40-ecacb162c214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Optimization reached maximum number of iterations"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hm.optimization_limits['num_cores'] = 2\n",
    "hm.optimization_limits['max_iter'] = 1000\n",
    "hm.optimization_limits['converged_min'] = 1e-7\n",
    "\n",
    "model_5s3d = hm.EM_H2MM_C(hm.factory_h2mm_model(4,3), color3, times3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff482be-2b50-45dc-8b42-ba670ac72fe3",
   "metadata": {},
   "source": [
    "## Hashable models *New v2.0*\n",
    "\n",
    "Version 2.0 introduced limited abilities to hash and use `h2mm_model`\n",
    "objects as keys in dictionaries.\n",
    "\n",
    "By default, most models cannot be used in this way,\n",
    "rather a model must first be put into a canonical form.\n",
    "To generate such a model, call the method `h2mm_model.sort_states`\n",
    "on any existing model. The returned model will be hashable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f77cea2-b166-482a-a6b3-f3b30779e3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835756030495084663"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_model_5s3d = model_5s3d.sort_states()\n",
    "hash(sorted_model_5s3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd78e7b-9c1d-4efb-bee6-317ffc4c0b91",
   "metadata": {},
   "source": [
    "Along with the hash function, h2mm_models can also be compared\n",
    "to one another with the equality operator. When two models have\n",
    "identical cannonical forms, they will evaluate to `True` in \n",
    "equality comparisons.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> This comparison is very strict, ie it is **not** an\n",
    "> almost equal comparison.\n",
    "> This is so that using hashable models as dictionary\n",
    "> keys will still function.\n",
    "> The floating point numbers in two models must be\n",
    "> identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6915657-a524-4a00-ac99-c5565019157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorteted model are equivalent to unsroted models\n",
      "unsoreted models are comparable, but not identical\n"
     ]
    }
   ],
   "source": [
    "if sorted_model_5s3d == model_5s3d:\n",
    "    print(\"sorteted model are equivalent to unsroted models\")\n",
    "else:\n",
    "    print(\"sorteted model are not equivalent to unsroted models\")\n",
    "model_dummy = hm.factory_h2mm_model(4,3)\n",
    "if model_dummy == model_5s3d:\n",
    "    print(\"oops, unsorted models are comparable, and identical\")\n",
    "else:\n",
    "    print(\"unsoreted models are comparable, but not identical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90542094-a8e5-49fb-a8c9-dc33d3059e95",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> Once a model is sorted so it can be hashed, it is fixed.\n",
    "> It can no longer be assigned new `prior`/`trans`/`obs`\n",
    "> values, and optimizations must be performed with `inplace=False`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
